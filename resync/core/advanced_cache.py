"""
Advanced Application Cache with Intelligent Invalidation.

This module provides an intelligent caching system with:
- Dependency-based automatic invalidation
- Dynamic TTL based on usage patterns
- Hierarchical caching (memory -> Redis -> database)
- Cache warming and predictive loading
- Performance metrics and monitoring
- Cascade invalidation for related data
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set, Tuple, Union

from resync.core.redis_init import get_redis_initializer
from resync.core.structured_logger import get_logger

logger = get_logger(__name__)


@dataclass
class CacheEntry:
    """Enhanced cache entry with metadata."""

    key: str
    value: Any
    ttl: int
    created_at: float = field(default_factory=time.time)
    last_accessed: float = field(default_factory=time.time)
    access_count: int = 0
    dependencies: Set[str] = field(default_factory=set)
    tags: Set[str] = field(default_factory=set)
    hit_rate: float = 1.0
    size_bytes: int = 0

    @property
    def age(self) -> float:
        """Get entry age in seconds."""
        return time.time() - self.created_at

    @property
    def idle_time(self) -> float:
        """Get idle time since last access."""
        return time.time() - self.last_accessed

    @property
    def is_expired(self) -> bool:
        """Check if entry is expired."""
        return self.age > self.ttl

    @property
    def should_evict(self) -> bool:
        """Check if entry should be evicted based on policies."""
        # Evict if expired
        if self.is_expired:
            return True

        # Evict if low hit rate and old
        if self.hit_rate < 0.1 and self.age > 3600:  # 1 hour
            return True

        return False

    def calculate_dynamic_ttl(self, usage_stats: Dict[str, Any]) -> int:
        """Calculate dynamic TTL based on usage patterns."""
        base_ttl = self.ttl

        # Increase TTL for frequently accessed items
        if self.hit_rate > 0.8 and self.access_count > 100:
            base_ttl *= 2

        # Decrease TTL for rarely accessed items
        elif self.hit_rate < 0.2:
            base_ttl = max(60, base_ttl // 4)  # Minimum 1 minute

        # Adjust based on access frequency
        access_freq = self.access_count / max(1, self.age / 3600)  # accesses per hour
        if access_freq > 10:  # High frequency
            base_ttl = min(base_ttl * 1.5, 86400)  # Max 24 hours
        elif access_freq < 1:  # Low frequency
            base_ttl = max(60, base_ttl // 2)  # Minimum 1 minute

        return int(base_ttl)


@dataclass
class CacheStats:
    """Comprehensive cache statistics."""

    # Hit/miss statistics
    hits: int = 0
    misses: int = 0
    total_requests: int = 0

    # Size and capacity
    current_entries: int = 0
    max_entries: int = 10000
    memory_usage_bytes: int = 0
    max_memory_bytes: int = 100 * 1024 * 1024  # 100MB

    # Performance metrics
    avg_hit_latency: float = 0.0
    avg_miss_latency: float = 0.0
    eviction_count: int = 0
    invalidation_count: int = 0

    # Layer statistics
    memory_hits: int = 0
    redis_hits: int = 0
    database_hits: int = 0

    def hit_rate(self) -> float:
        """Calculate cache hit rate."""
        return self.hits / max(1, self.total_requests)

    def memory_efficiency(self) -> float:
        """Calculate memory efficiency."""
        return self.memory_usage_bytes / max(1, self.max_memory_bytes)


@dataclass
class InvalidationRule:
    """Rule for automatic cache invalidation."""

    pattern: str  # Key pattern to match
    dependencies: List[str]  # Keys that depend on this pattern
    cascade: bool = True  # Whether to invalidate recursively
    ttl_multiplier: float = 1.0  # TTL adjustment for dependent keys


class CacheDependencyGraph:
    """Graph for managing cache dependencies and cascade invalidation."""

    def __init__(self):
        self.dependencies: Dict[str, Set[str]] = defaultdict(set)  # key -> dependents
        self.reverse_dependencies: Dict[str, Set[str]] = defaultdict(
            set
        )  # key -> dependencies
        self.tags: Dict[str, Set[str]] = defaultdict(set)  # tag -> keys

    def add_dependency(self, key: str, depends_on: str) -> None:
        """Add a dependency relationship."""
        self.dependencies[depends_on].add(key)
        self.reverse_dependencies[key].add(depends_on)

    def add_tags(self, key: str, tags: List[str]) -> None:
        """Add tags to a key."""
        for tag in tags:
            self.tags[tag].add(key)

    def get_dependents(self, key: str) -> Set[str]:
        """Get all keys that depend on the given key."""
        return self.dependencies.get(key, set())

    def get_dependencies(self, key: str) -> Set[str]:
        """Get all keys that the given key depends on."""
        return self.reverse_dependencies.get(key, set())

    def get_keys_by_tag(self, tag: str) -> Set[str]:
        """Get all keys with a specific tag."""
        return self.tags.get(tag, set())

    def cascade_invalidate(self, key: str) -> Set[str]:
        """Get all keys that should be invalidated due to cascade."""
        to_invalidate = set()
        visited = set()

        def _collect_dependents(current_key: str) -> None:
            if current_key in visited:
                return
            visited.add(current_key)

            # Add direct dependents
            for dependent in self.get_dependents(current_key):
                to_invalidate.add(dependent)
                _collect_dependents(dependent)

        _collect_dependents(key)
        return to_invalidate

    def remove_key(self, key: str) -> None:
        """Remove a key from the dependency graph."""
        # Remove from dependencies
        for dependents in self.dependencies.values():
            dependents.discard(key)

        # Remove from reverse dependencies
        if key in self.reverse_dependencies:
            del self.reverse_dependencies[key]

        # Remove from tags
        for tag_keys in self.tags.values():
            tag_keys.discard(key)


class AdvancedCacheManager:
    """
    Intelligent cache manager with automatic invalidation and hierarchical caching.

    Features:
    - Multi-layer caching (memory -> Redis -> database)
    - Dependency-based invalidation
    - Dynamic TTL adjustment
    - Cache warming and predictive loading
    - Performance monitoring and analytics
    """

    def __init__(self):
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.stats = CacheStats()
        self.dependency_graph = CacheDependencyGraph()

        # Redis integration
        self.redis_client = None
        self.redis_enabled = False

        # Background tasks
        self._cleanup_task: Optional[asyncio.Task] = None
        self._warming_task: Optional[asyncio.Task] = None
        self._running = False

        # Configuration
        self.ttl_policies = {
            "user_data": 300,  # 5 minutes
            "config": 3600,  # 1 hour
            "analytics": 1800,  # 30 minutes
            "api_responses": 600,  # 10 minutes
            "static_data": 86400,  # 24 hours
        }

        # Invalidation rules
        self.invalidation_rules: List[InvalidationRule] = []

        # Thread safety
        self._lock = asyncio.Lock()

    async def initialize(self) -> None:
        """Initialize the advanced cache manager."""
        if self._running:
            return

        # Initialize Redis client
        try:
            redis_init = await get_redis_initializer()
            self.redis_client = await redis_init.initialize()
            self.redis_enabled = True
            logger.info("Redis integration enabled for advanced caching")
        except Exception as e:
            logger.warning(f"Redis not available for caching: {e}")
            self.redis_enabled = False

        self._running = True

        # Start background tasks
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        self._warming_task = asyncio.create_task(self._warming_loop())

        logger.info("Advanced cache manager initialized")

    async def shutdown(self) -> None:
        """Shutdown the cache manager."""
        if not self._running:
            return

        self._running = False

        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        if self._warming_task:
            self._warming_task.cancel()
            try:
                await self._warming_task
            except asyncio.CancelledError:
                pass

        # Clear caches
        async with self._lock:
            self.memory_cache.clear()
            if self.redis_client:
                # Clear Redis cache (optional - depends on use case)
                pass

        logger.info("Advanced cache manager shutdown")

    async def get(
        self,
        key: str,
        fetch_func: Optional[callable] = None,
        ttl: Optional[int] = None,
        dependencies: Optional[List[str]] = None,
        tags: Optional[List[str]] = None,
    ) -> Any:
        """
        Get value from cache with intelligent fallback.

        Args:
            key: Cache key
            fetch_func: Function to fetch data if not cached
            ttl: Time to live in seconds
            dependencies: Keys this entry depends on
            tags: Tags for grouping and invalidation

        Returns:
            Cached or fetched value
        """
        start_time = time.time()

        # Try memory cache first
        value = await self._get_from_memory(key)
        if value is not None:
            self._record_hit("memory", time.time() - start_time)
            return value

        # Try Redis cache
        if self.redis_enabled:
            value = await self._get_from_redis(key)
            if value is not None:
                # Promote to memory cache
                await self._set_memory(key, value, ttl or 300, dependencies, tags)
                self._record_hit("redis", time.time() - start_time)
                return value

        # Cache miss - fetch from source
        if fetch_func:
            try:
                value = await fetch_func()
                # Cache the result
                await self.set(key, value, ttl, dependencies, tags)
                self._record_miss(time.time() - start_time)
                return value
            except Exception as e:
                logger.error(f"Failed to fetch data for key {key}: {e}")
                self._record_miss(time.time() - start_time)
                raise

        self._record_miss(time.time() - start_time)
        return None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: Optional[int] = None,
        dependencies: Optional[List[str]] = None,
        tags: Optional[List[str]] = None,
    ) -> None:
        """Set value in cache with metadata."""
        # Determine TTL
        if ttl is None:
            ttl = self._calculate_ttl(key, value)

        # Create cache entry
        entry = CacheEntry(
            key=key,
            value=value,
            ttl=ttl,
            dependencies=set(dependencies or []),
            tags=set(tags or []),
        )

        # Calculate size
        entry.size_bytes = self._calculate_size(value)

        # Set in memory
        await self._set_memory_entry(entry)

        # Set in Redis if enabled
        if self.redis_enabled:
            await self._set_redis(key, value, ttl)

        # Update dependency graph
        if dependencies:
            for dep in dependencies:
                self.dependency_graph.add_dependency(key, dep)

        if tags:
            self.dependency_graph.add_tags(key, tags)

        # Update stats
        async with self._lock:
            self.stats.current_entries = len(self.memory_cache)
            self.stats.memory_usage_bytes = sum(
                entry.size_bytes for entry in self.memory_cache.values()
            )

    async def invalidate(self, key: str, cascade: bool = True) -> int:
        """
        Invalidate cache entry with optional cascade invalidation.

        Returns:
            Number of entries invalidated
        """
        invalidated = 0

        async with self._lock:
            # Direct invalidation
            if key in self.memory_cache:
                del self.memory_cache[key]
                invalidated += 1

            if self.redis_enabled:
                await self.redis_client.delete(key)

            # Cascade invalidation
            if cascade:
                cascade_keys = self.dependency_graph.cascade_invalidate(key)
                for cascade_key in cascade_keys:
                    if cascade_key in self.memory_cache:
                        del self.memory_cache[cascade_key]
                        invalidated += 1

                    if self.redis_enabled:
                        await self.redis_client.delete(cascade_key)

                # Clean up dependency graph
                self.dependency_graph.remove_key(key)
                for cascade_key in cascade_keys:
                    self.dependency_graph.remove_key(cascade_key)

        self.stats.invalidation_count += invalidated
        logger.info(f"Invalidated {invalidated} cache entries for key {key}")
        return invalidated

    async def invalidate_by_tag(self, tag: str) -> int:
        """Invalidate all entries with a specific tag."""
        keys_to_invalidate = self.dependency_graph.get_keys_by_tag(tag)
        invalidated = 0

        for key in keys_to_invalidate:
            invalidated += await self.invalidate(key, cascade=False)

        logger.info(f"Invalidated {invalidated} entries with tag {tag}")
        return invalidated

    async def invalidate_by_pattern(self, pattern: str) -> int:
        """Invalidate entries matching a pattern."""
        import re

        regex = re.compile(pattern)

        keys_to_invalidate = [
            key for key in self.memory_cache.keys() if regex.match(key)
        ]

        invalidated = 0
        for key in keys_to_invalidate:
            invalidated += await self.invalidate(key, cascade=False)

        logger.info(f"Invalidated {invalidated} entries matching pattern {pattern}")
        return invalidated

    async def warm_cache(self, warming_keys: List[Dict[str, Any]]) -> int:
        """
        Warm cache with predefined keys.

        Args:
            warming_keys: List of dicts with keys: 'key', 'fetch_func', 'ttl', 'dependencies', 'tags'

        Returns:
            Number of keys warmed
        """
        warmed = 0

        for key_config in warming_keys:
            try:
                key = key_config["key"]
                fetch_func = key_config["fetch_func"]

                # Check if already cached
                existing = await self._get_from_memory(key)
                if existing is not None:
                    continue

                # Fetch and cache
                value = await fetch_func()
                await self.set(
                    key=key,
                    value=value,
                    ttl=key_config.get("ttl"),
                    dependencies=key_config.get("dependencies"),
                    tags=key_config.get("tags"),
                )

                warmed += 1
                logger.debug(f"Warmed cache key: {key}")

            except Exception as e:
                logger.warning(
                    f"Failed to warm cache key {key_config.get('key', 'unknown')}: {e}"
                )

        logger.info(f"Cache warming completed: {warmed} keys warmed")
        return warmed

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics."""
        return {
            "performance": {
                "hit_rate": self.stats.hit_rate(),
                "total_requests": self.stats.total_requests,
                "hits": self.stats.hits,
                "misses": self.stats.misses,
                "avg_hit_latency": self.stats.avg_hit_latency,
                "avg_miss_latency": self.stats.avg_miss_latency,
            },
            "capacity": {
                "current_entries": self.stats.current_entries,
                "max_entries": self.stats.max_entries,
                "memory_usage_mb": self.stats.memory_usage_bytes / (1024 * 1024),
                "max_memory_mb": self.stats.max_memory_bytes / (1024 * 1024),
                "memory_efficiency": self.stats.memory_efficiency(),
            },
            "operations": {
                "evictions": self.stats.eviction_count,
                "invalidations": self.stats.invalidation_count,
                "memory_hits": self.stats.memory_hits,
                "redis_hits": self.stats.redis_hits,
                "database_hits": self.stats.database_hits,
            },
            "layers": {
                "memory_enabled": True,
                "redis_enabled": self.redis_enabled,
                "database_enabled": True,
            },
        }

    async def _get_from_memory(self, key: str) -> Any:
        """Get value from memory cache."""
        async with self._lock:
            if key in self.memory_cache:
                entry = self.memory_cache[key]

                if entry.is_expired:
                    del self.memory_cache[key]
                    return None

                # Update access statistics
                entry.last_accessed = time.time()
                entry.access_count += 1
                entry.hit_rate = entry.access_count / max(
                    1, entry.age / 60
                )  # hits per minute

                return entry.value

        return None

    async def _set_memory_entry(self, entry: CacheEntry) -> None:
        """Set entry in memory cache with eviction if needed."""
        async with self._lock:
            # Check size limits
            if len(self.memory_cache) >= self.stats.max_entries:
                await self._evict_entries()

            if (
                self.stats.memory_usage_bytes + entry.size_bytes
                > self.stats.max_memory_bytes
            ):
                await self._evict_entries()

            self.memory_cache[entry.key] = entry

    async def _get_from_redis(self, key: str) -> Any:
        """Get value from Redis cache."""
        if not self.redis_enabled or not self.redis_client:
            return None

        try:
            value_json = await self.redis_client.get(key)
            if value_json:
                return json.loads(value_json)
        except Exception as e:
            logger.warning(f"Redis get error for key {key}: {e}")

        return None

    async def _set_redis(self, key: str, value: Any, ttl: int) -> None:
        """Set value in Redis cache."""
        if not self.redis_enabled or not self.redis_client:
            return

        try:
            value_json = json.dumps(value)
            await self.redis_client.set(key, value_json, ex=ttl)
        except Exception as e:
            logger.warning(f"Redis set error for key {key}: {e}")

    def _calculate_ttl(self, key: str, value: Any) -> int:
        """Calculate appropriate TTL based on key patterns and value characteristics."""
        # Check if key matches known patterns
        for pattern, ttl in self.ttl_policies.items():
            if pattern in key.lower():
                return ttl

        # Default TTL based on value type/size
        if isinstance(value, (list, dict)) and len(str(value)) > 1000:
            return 1800  # Large data - 30 minutes
        elif isinstance(value, (int, float, str)) and len(str(value)) < 100:
            return 3600  # Small data - 1 hour
        else:
            return 900  # Default - 15 minutes

    def _calculate_size(self, value: Any) -> int:
        """Calculate approximate memory usage of a value."""
        return len(json.dumps(value, default=str).encode("utf-8"))

    async def _evict_entries(self) -> None:
        """Evict entries based on LRU and other policies."""
        async with self._lock:
            # Sort entries by eviction priority (age, hit rate, size)
            entries = list(self.memory_cache.items())

            def eviction_score(entry_tuple):
                key, entry = entry_tuple
                # Lower score = more likely to evict
                score = (
                    entry.idle_time * (2 - entry.hit_rate) * (entry.size_bytes / 1000)
                )
                return score

            entries.sort(key=eviction_score, reverse=True)  # Highest scores first

            # Evict 10% of entries or at least 1
            to_evict = max(1, len(entries) // 10)

            for i in range(to_evict):
                if i < len(entries):
                    key, entry = entries[i]
                    if key in self.memory_cache:
                        del self.memory_cache[key]
                        self.stats.eviction_count += 1
                        self.dependency_graph.remove_key(key)

    def _record_hit(self, layer: str, latency: float) -> None:
        """Record cache hit."""
        self.stats.hits += 1
        self.stats.total_requests += 1

        # Update latency
        if self.stats.hits == 1:
            self.stats.avg_hit_latency = latency
        else:
            self.stats.avg_hit_latency = (
                (self.stats.avg_hit_latency * (self.stats.hits - 1)) + latency
            ) / self.stats.hits

        # Layer-specific hits
        if layer == "memory":
            self.stats.memory_hits += 1
        elif layer == "redis":
            self.stats.redis_hits += 1
        elif layer == "database":
            self.stats.database_hits += 1

    def _record_miss(self, latency: float) -> None:
        """Record cache miss."""
        self.stats.misses += 1
        self.stats.total_requests += 1

        # Update latency
        if self.stats.misses == 1:
            self.stats.avg_miss_latency = latency
        else:
            self.stats.avg_miss_latency = (
                (self.stats.avg_miss_latency * (self.stats.misses - 1)) + latency
            ) / self.stats.misses

    async def _cleanup_loop(self) -> None:
        """Background cleanup of expired entries."""
        while self._running:
            try:
                await asyncio.sleep(60)  # Clean every minute

                async with self._lock:
                    expired_keys = [
                        key
                        for key, entry in self.memory_cache.items()
                        if entry.should_evict()
                    ]

                    for key in expired_keys:
                        if key in self.memory_cache:
                            del self.memory_cache[key]
                            self.stats.eviction_count += 1
                            self.dependency_graph.remove_key(key)

                    if expired_keys:
                        logger.debug(
                            f"Cleaned up {len(expired_keys)} expired cache entries"
                        )

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Cache cleanup error: {e}")

    async def _warming_loop(self) -> None:
        """Background cache warming based on access patterns."""
        while self._running:
            try:
                await asyncio.sleep(300)  # Check every 5 minutes

                # Identify frequently accessed keys that could benefit from longer TTL
                async with self._lock:
                    candidates = [
                        (key, entry)
                        for key, entry in self.memory_cache.items()
                        if entry.hit_rate > 0.5 and entry.access_count > 10
                    ]

                    for key, entry in candidates:
                        # Extend TTL for hot entries
                        new_ttl = entry.calculate_dynamic_ttl({})
                        if new_ttl > entry.ttl:
                            entry.ttl = new_ttl
                            logger.debug(
                                f"Extended TTL for hot key {key} to {new_ttl}s"
                            )

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Cache warming error: {e}")


# Global cache manager instance
advanced_cache_manager = AdvancedCacheManager()


async def get_advanced_cache_manager() -> AdvancedCacheManager:
    """Get the global advanced cache manager instance."""
    if not advanced_cache_manager._running:
        await advanced_cache_manager.initialize()
    return advanced_cache_manager
