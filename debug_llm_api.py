#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Debug detalhado da configuraÃ§Ã£o e conectividade LLM API
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

async def debug_llm_configuration():
    """Debug completo da configuraÃ§Ã£o LLM"""
    print("ğŸ” DEBUG DETALHADO DA CONFIGURAÃ‡ÃƒO LLM")
    print("=" * 60)
    
    try:
        # Import settings
        from resync.settings import settings
        
        print("ğŸ“‹ CONFIGURAÃ‡ÃƒO ATUAL:")
        print(f"   LLM Endpoint: {settings.llm_endpoint}")
        print(f"   LLM API Key: {settings.llm_api_key[:20]}...{settings.llm_api_key[-10:]}")
        print(f"   LLM Timeout: {settings.llm_timeout}s")
        print(f"   Environment: {settings.environment}")
        
        # Verificar configuraÃ§Ãµes do .env
        print("\nğŸ“ ARQUIVO .ENV:")
        env_file = Path('.env')
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for line in lines:
                    if any(key in line.upper() for key in ['LLM_', 'ENDPOINT', 'API_KEY', 'MODEL']):
                        print(f"   {line.strip()}")
        else:
            print("   âŒ Arquivo .env nÃ£o encontrado")
        
        # Testar LLM Service
        print("\nğŸ§ª TESTE DO LLM SERVICE:")
        from resync.services.llm_service import get_llm_service
        
        llm_service = get_llm_service()
        print(f"   âœ… LLM Service inicializado")
        print(f"   ğŸ“± Modelo: {llm_service.model}")
        print(f"   ğŸŒ Base URL: {llm_service.client.base_url}")
        print(f"   ğŸ”‘ API Key: {llm_service.client.api_key[:20]}...{llm_service.client.api_key[-10:]}")
        
        # Teste de conectividade bÃ¡sica
        print("\nğŸŒ TESTE DE CONECTIVIDADE:")
        try:
            import aiohttp
            
            # Testar endpoint base
            async with aiohttp.ClientSession() as session:
                headers = {
                    'Authorization': f'Bearer {settings.llm_api_key}',
                    'Content-Type': 'application/json'
                }
                
                print(f"   ğŸ“¡ Testando endpoint: {settings.llm_endpoint}")
                
                # Testar se endpoint responde
                async with session.get(f"{settings.llm_endpoint}/models", headers=headers) as response:
                    print(f"   ğŸ“Š Status Code: {response.status}")
                    if response.status == 200:
                        models = await response.json()
                        print(f"   âœ… Endpoint respondeu - {len(models.get('data', []))} modelos disponÃ­veis")
                        for model in models.get('data', [])[:3]:
                            print(f"      - {model.get('id', 'Unknown')}")
                    else:
                        error_text = await response.text()
                        print(f"   âŒ Erro: {error_text}")
                        
        except Exception as e:
            print(f"   âŒ Erro de conectividade: {e}")
        
        # Teste de health check do LLM service
        print("\nğŸ¥ HEALTH CHECK DO LLM:")
        try:
            health_status = await llm_service.health_check()
            print(f"   Status: {health_status['status']}")
            print(f"   Model: {health_status['model']}")
            print(f"   Endpoint: {health_status['endpoint']}")
            
            if 'error' in health_status:
                print(f"   âŒ Erro: {health_status['error']}")
            elif 'test_response' in health_status:
                print(f"   âœ… Test Response: {health_status['test_response']}")
                
        except Exception as e:
            print(f"   âŒ Erro no health check: {e}")
        
        # Teste de geraÃ§Ã£o de resposta
        print("\nğŸ’¬ TESTE DE GERAÃ‡ÃƒO DE RESPOSTA:")
        try:
            messages = [
                {"role": "user", "content": "Responda apenas com 'OK' para teste."}
            ]
            
            response = await llm_service.generate_response(messages, max_tokens=10)
            print(f"   âœ… Resposta gerada: {response}")
            
        except Exception as e:
            print(f"   âŒ Erro na geraÃ§Ã£o: {e}")
            print(f"   ğŸ“ Tipo do erro: {type(e).__name__}")
            if hasattr(e, 'details'):
                print(f"   ğŸ“‹ Detalhes: {e.details}")
        
        # AnÃ¡lise final
        print("\nğŸ“Š ANÃLISE FINAL:")
        if settings.llm_endpoint.startswith("https://integrate.api.nvidia.com"):
            print("   ğŸ” Usando NVIDIA API")
            print("   âš ï¸  Verifique se a API key Ã© vÃ¡lida para NVIDIA")
        elif settings.llm_endpoint.startswith("https://openrouter.ai"):
            print("   ğŸ” Usando OpenRouter API")
            print("   âš ï¸  Verifique se a API key Ã© vÃ¡lida para OpenRouter")
        else:
            print(f"   ğŸ” Usando endpoint customizado: {settings.llm_endpoint}")
            
        print("\nğŸ’¡ RECOMENDAÃ‡Ã•ES:")
        print("   1. Verifique se a API key estÃ¡ correta e ativa")
        print("   2. Confirme se o endpoint estÃ¡ correto para o provedor")
        print("   3. Verifique limites de uso da API")
        print("   4. Teste com curl ou Postman para isolar o problema")
        
    except Exception as e:
        print(f"âŒ Erro geral no debug: {e}")
        import traceback
        traceback.print_exc()

async def test_api_direct():
    """Teste direto da API com diferentes configuraÃ§Ãµes"""
    print("\nğŸ”§ TESTE DIRETO DA API (MÃšLTIPLAS CONFIGS)")
    print("=" * 60)
    
    # ConfiguraÃ§Ãµes para testar
    configs = [
        {
            "name": "NVIDIA API (config .env)",
            "endpoint": "https://integrate.api.nvidia.com/v1",
            "api_key": "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"
        },
        {
            "name": "OpenRouter (settings.py)",
            "endpoint": "https://openrouter.ai/api/v1",
            "api_key": "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"
        }
    ]
    
    try:
        from openai import AsyncOpenAI
        
        for config in configs:
            print(f"\nğŸ“¡ Testando: {config['name']}")
            print(f"   Endpoint: {config['endpoint']}")
            print(f"   API Key: {config['api_key'][:20]}...{config['api_key'][-10:]}")
            
            try:
                client = AsyncOpenAI(
                    api_key=config['api_key'],
                    base_url=config['endpoint']
                )
                
                # Teste simples
                response = await client.chat.completions.create(
                    model="gpt-3.5-turbo" if "openrouter" in config['endpoint'] else "nvidia/llama-3.3-nemotron-super-49b-v1.5",
                    messages=[{"role": "user", "content": "Responda apenas com 'OK'."}],
                    max_tokens=10
                )
                
                result = response.choices[0].message.content
                print(f"   âœ… Sucesso: {result}")
                
            except Exception as e:
                print(f"   âŒ Falha: {e}")
                print(f"   ğŸ“ Tipo: {type(e).__name__}")
                
    except ImportError:
        print("   âŒ Biblioteca openai nÃ£o instalada")

if __name__ == "__main__":
    asyncio.run(debug_llm_configuration())
    asyncio.run(test_api_direct())
